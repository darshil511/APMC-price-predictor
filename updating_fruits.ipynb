{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def write_to_csv_parameters(filename):\n",
    "    fieldnames = ['Date', 'Item Name', 'p', 'd', 'q']\n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write header row\n",
    "        writer.writerow(fieldnames)\n",
    "        \n",
    "        # writer.writerow([today_date, product_name, p, d, q])\n",
    "        \n",
    "base_dir = Path(os.getenv(\"BASE_DIRECTORY\"))\n",
    "filename = base_dir / \"ml_models/fruits_saved_models/fruits_parameters.csv\"\n",
    "write_to_csv_parameters(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from dateutil import rrule\n",
    "from datetime import date\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from scipy.stats import boxcox, skew\n",
    "from matplotlib import font_manager as fm\n",
    "import os\n",
    "import joblib\n",
    "import hashlib\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "products_with_boxcox = []\n",
    "\n",
    "\n",
    "### removes data older than 4 months-----------------------------------------------------------------------------------------------\n",
    "base_dir = Path(os.getenv(\"BASE_DIRECTORY\"))\n",
    "file_path = base_dir / \"data/fruits/fruits_price_data.csv\"\n",
    "# csv_file = \"commodities_price_data.csv\"  # Update with your file path\n",
    "save_dir = base_dir / \"ml_models/fruits_saved_models\"\n",
    "filename = base_dir / \"ml_models/fruits_saved_models/fruits_parameters.csv\"\n",
    "allYearsFilename = base_dir / \"data/fruits/fruits_price_data-all_years.csv\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "data = pd.read_csv(file_path, encoding='utf-8', parse_dates=['Date'], date_format='%d/%m/%Y')\n",
    "print(data.head())\n",
    "products = data['Item Name'].unique()\n",
    "print(products)\n",
    "\n",
    "# Get current date and calculate the cutoff date (4 months ago)\n",
    "current_date = datetime.today()\n",
    "cutoff_date = current_date - timedelta(days=4*30) # Approximate 4 months\n",
    "cutoff_date = cutoff_date.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "today_date = datetime.now().strftime(\"%d/%m/%Y\")\n",
    "\n",
    "# Filter data to keep only the last 4 months\n",
    "filtered_data = data[pd.to_datetime(data[\"Date\"], dayfirst=True) >= cutoff_date]\n",
    "\n",
    "# Save the cleaned data back to CSV\n",
    "filtered_data.to_csv(file_path, encoding='utf-8', index=False)\n",
    "\n",
    "print(f\"Data older than {cutoff_date.strftime('%d-%m-%Y')} has been removed.\")\n",
    "\n",
    "logs = []\n",
    "###------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#function to get product parameters from csv\n",
    "def get_product_parameters(filename, product_name):\n",
    "    result = []\n",
    "    \n",
    "    # Open the CSV file\n",
    "    with open(filename, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)  # Using DictReader to work with column headers\n",
    "        for row in reader:\n",
    "            if product_name in row['Item Name']:  # Check if product_name matches\n",
    "                result.append({\n",
    "                    'Date': row['Date'],\n",
    "                    'Item Name': row['Item Name'],\n",
    "                    'p': row['p'],\n",
    "                    'd': row['d'],\n",
    "                    'q': row['q']\n",
    "                })\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "#function to update arima model parameters\n",
    "def update_product_parameters(csv_file, today_date, product_name, p, d, q):\n",
    "    # Read the existing CSV into a list of rows\n",
    "    rows = []\n",
    "    header = []\n",
    "    \n",
    "    # Open the CSV to read its current data\n",
    "    if os.path.exists(csv_file):\n",
    "        with open(csv_file, mode='r', newline='', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            # Read the header and the rest of the rows\n",
    "            header = next(reader)  # The first row is the header\n",
    "            rows = list(reader)  # The rest of the rows are the data\n",
    "    else:\n",
    "        # If the file doesn't exist, we initialize an empty list for rows\n",
    "        rows = []\n",
    "    \n",
    "    # Flag to check if the product is found\n",
    "    product_found = False\n",
    "    \n",
    "    # Iterate through the rows and update the (p, d, q) values if the product is found\n",
    "    for i, row in enumerate(rows):\n",
    "        if row[1] == product_name:  # Assuming the 'Item Name' is in the second column (index 1)\n",
    "            rows[i] = [today_date, product_name, p, d, q]  # Update the row with new values\n",
    "            product_found = True\n",
    "            message = f\"Updated {product_name} with new (p, d, q): ({p}, {d}, {q})\"\n",
    "            # print(message)\n",
    "            logs.append(message)\n",
    "            break\n",
    "    \n",
    "    # If the product was not found, append it as a new entry\n",
    "    if not product_found:\n",
    "        rows.append([today_date, product_name, p, d, q])  # Add a new row for the product\n",
    "        message = f\"Added new item {product_name} with (p, d, q): ({p}, {d}, {q})\"\n",
    "        # print(message)\n",
    "        logs.append(message)\n",
    "    \n",
    "    # Write the updated data back to the CSV\n",
    "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write the header first\n",
    "        writer.writerow(header)\n",
    "        # Write all the rows (updated or new)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "\n",
    "\n",
    "###Saving updated ARIMA models---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Step 1: Stationarity Check\n",
    "def check_stationarity(series, size):\n",
    "    if len(series) == size:\n",
    "        return True\n",
    "    result = adfuller(series)\n",
    "    print(\"\\nADF Test Results:\")\n",
    "    print(f\"ADF Statistic: {result[0]:.4f}\")\n",
    "    print(f\"p-value: {result[1]:.4f}\")\n",
    "    for key, value in result[4].items():\n",
    "        print(f\"Critical Value {key}: {value:.4f}\")\n",
    "    if result[1] < 0.05:\n",
    "        print(\"The series is stationary (no further differencing needed).\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"The series is not stationary (differencing is required).\")\n",
    "        return False\n",
    "\n",
    "# Function to count significant lags\n",
    "def count_significant_spikes(values, confint):\n",
    "    # The significant lags are those outside the confidence interval\n",
    "    significant_lags = np.where(np.abs(values) > confint[:, 1])[0]\n",
    "    return len(significant_lags), significant_lags\n",
    "\n",
    "# Function to generate a safe file name\n",
    "def safe_filename(product_name):\n",
    "    return hashlib.md5(product_name.encode('utf-8')).hexdigest()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###updating the csv with current data---------------------------------------------------------------------------------------------\n",
    "\n",
    "def write_to_csv(data, filename):\n",
    "    # Define the field names for the CSV file\n",
    "    # fieldnames = ['Serial Number', 'Date', 'Item Name', 'Low Price', 'Average Price', 'High Price']\n",
    "    \n",
    "    # Write data to CSV file\n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write header row\n",
    "        # writer.writerow(fieldnames)\n",
    "        \n",
    "        # Write data rows\n",
    "        for _, row_data in enumerate(data):\n",
    "            date = row_data[0]\n",
    "            item_name = row_data[2]\n",
    "            low_price = row_data[3]\n",
    "            avg_price = row_data[5]\n",
    "            high_price = row_data[4]\n",
    "            writer.writerow([date, item_name, low_price, avg_price, high_price])\n",
    "            \n",
    "            \n",
    "def getData(driver, csv_file_name, webpage_path, date_path, submit_button_path, table_path):\n",
    "    \n",
    "    # add_field_names_price(csv_file_name)\n",
    "    \n",
    "    #iterate over all the dates    \n",
    "    try:\n",
    "        # Open the webpage\n",
    "        driver.get(webpage_path)\n",
    "            \n",
    "        # Find the date input element\n",
    "        date_input = driver.find_element(By.XPATH, date_path)\n",
    "\n",
    "        # Clear any existing text in the date input field\n",
    "        date_input.clear()\n",
    "          \n",
    "        # Enter the desired date\n",
    "        date_input.send_keys(current_date.strftime('%d-%m-%Y'))  # Example date, replace with your desired date\n",
    "\n",
    "        # Find and click the button to get data for the specified date\n",
    "        submit_button = driver.find_element(By.XPATH, submit_button_path)\n",
    "        submit_button.click()\n",
    "         \n",
    "        time.sleep(3) #to load data\n",
    "\n",
    "        #Extract the table data\n",
    "        table = driver.find_element(By.XPATH, table_path)\n",
    "        # print(table)\n",
    "        data = []\n",
    "\n",
    "        # Iterate over each row in the table\n",
    "        for row in table.find_elements(By.TAG_NAME, \"tr\"):\n",
    "            # Find all cells (td) in the row\n",
    "            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                \n",
    "            # Ensure that there are cells in the row\n",
    "            if cells:\n",
    "                # Extract text from each cell and append to the data list\n",
    "                row_data = [current_date.strftime('%d-%m-%Y')] + [cell.text.strip() for cell in cells]\n",
    "                data.append(row_data)\n",
    "\n",
    "        # Print the extracted data for debugging\n",
    "        # for row in data:\n",
    "        #     print(row)\n",
    "        write_to_csv(data, csv_file_name)\n",
    "        write_to_csv(data, allYearsFilename)\n",
    "        time.sleep(2)\n",
    "        # return data\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Initialize Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # commodities_data_table = \n",
    "    getData(driver, file_path, os.getenv(\"FRUITS_WEBPAGE_PATH\"),\n",
    "                                           os.getenv(\"FRUITS_DATE_PATH\"),\n",
    "                                           os.getenv(\"FRUITS_SUBMIT_BUTTON_PATH\"),\n",
    "                                           os.getenv(\"FRUITS_TABLE_PATH\"))\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###Updating models------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    for product in products:\n",
    "    \n",
    "        product_name = product \n",
    "        product_data = data[data['Item Name'] == product_name]\n",
    "        size_product_data = product_data.shape[0]\n",
    "        print(product_name, \"size: \", size_product_data)\n",
    "        \n",
    "        if((size_product_data//2)-1 <= 0):\n",
    "            continue\n",
    "        \n",
    "        # Check if any data exists for the selected product\n",
    "        if product_data.empty:\n",
    "            print(f\"No data found for the product: {product_name}\")\n",
    "\n",
    "        else:\n",
    "            # Extract the 'Average Price' column for the selected product\n",
    "            price_data = product_data['Average Price']\n",
    "            # price_data.index = pd.to_datetime(price_data.index, format='%d-%m-%Y')\n",
    "            # Get the last date in the dataset\n",
    "            # latest_date = price_data.index.max()\n",
    "            #print(latest_date)\n",
    "\n",
    "            # Calculate the start date for the last 3 months\n",
    "            # three_months_ago = latest_date - pd.DateOffset(months=3)\n",
    "            #print(three_months_ago)\n",
    "\n",
    "            # Filter the data for plotting\n",
    "            # filtered_data = price_data[price_data.index >= three_months_ago]\n",
    "            #print(filtered_data)\n",
    "\n",
    "\n",
    "            # Plot the raw price data for the selected commodity\n",
    "            # plt.figure(figsize=(12, 6))\n",
    "            # plt.plot(price_data.index, price_data, label=f'{commodity_name} Prices', marker='o', linestyle='-')\n",
    "            # plt.title(f'Price Trend for {commodity_name}')\n",
    "            # plt.xlabel('Date')\n",
    "            # plt.ylabel('Average Price')\n",
    "            # plt.xticks(rotation=90)\n",
    "            # plt.legend()\n",
    "            # plt.grid()\n",
    "            # plt.tight_layout()\n",
    "            # plt.show()\n",
    "            \n",
    "        variance_ratio = price_data.std() / price_data.mean()\n",
    "        skewness = skew(price_data)\n",
    "\n",
    "        if variance_ratio > 0.1 and skewness > 0.5:\n",
    "            apply_boxcox = True\n",
    "            products_with_boxcox.append(product_name)\n",
    "        else:\n",
    "            apply_boxcox = False\n",
    "        \n",
    "        \n",
    "        print(apply_boxcox)\n",
    "        # Step 2: Differencing (if necessary)\n",
    "        print(f\"Checking stationarity for the raw price data of {product_name}:\")\n",
    "        d = 0\n",
    "        price_diff = price_data\n",
    "        \n",
    "        try:\n",
    "            if apply_boxcox:\n",
    "                price_diff, lam = boxcox(price_diff)\n",
    "                price_diff = pd.Series(price_diff)\n",
    "        \n",
    "            while True:\n",
    "                if check_stationarity(price_diff, (size_product_data//2)+1):\n",
    "                    # price_diff = price_data\n",
    "                    print(f\"Data is stationary with d={d}.\")\n",
    "                    break\n",
    "                else:\n",
    "                    price_diff = price_diff.diff().dropna()\n",
    "                    d += 1\n",
    "        \n",
    "        # if check_stationarity(price_data):\n",
    "        #   price_diff = price_data\n",
    "        # else:\n",
    "        #   price_diff = price_data.diff().dropna()  # Apply differencing to make the series stationary\n",
    "\n",
    "            print(\"\\nChecking stationarity for the differenced data:\")\n",
    "            check_stationarity(price_diff, (size_product_data//2)+1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Stationarity cannot be applicable for {product_name}\")\n",
    "        # print(d)\n",
    "\n",
    "        # filtered_price_diff = price_diff[price_diff.index >= three_months_ago]  # For differenced data\n",
    "\n",
    "        # Plot the differenced data\n",
    "        # plt.figure(figsize=(12, 6))\n",
    "        # plt.plot(price_diff, label='Differenced Data', marker='o', linestyle='-')\n",
    "        # plt.title('Differenced Data')\n",
    "        # plt.xlabel('Date')\n",
    "        # plt.ylabel('Price Difference')\n",
    "        # plt.xticks(rotation=90)\n",
    "        # plt.legend()\n",
    "        # plt.grid()\n",
    "        # plt.show()\n",
    "        \n",
    "        \n",
    "        # Step 3: Plot ACF and PACF\n",
    "\n",
    "        acf_values, acf_confint = acf(price_diff, alpha=0.05)\n",
    "        pacf_values, pacf_confint = pacf(price_diff, alpha=0.05)\n",
    "\n",
    "\n",
    "        # Count significant spikes for ACF (q)\n",
    "        q, significant_acf_lags = count_significant_spikes(acf_values, acf_confint)\n",
    "\n",
    "        # Count significant spikes for PACF (p)\n",
    "        p, significant_pacf_lags = count_significant_spikes(pacf_values, pacf_confint)\n",
    "\n",
    "        print(f\"Significant spikes in ACF (q): {q} at lags {significant_acf_lags}\")\n",
    "        print(f\"Significant spikes in PACF (p): {p} at lags {significant_pacf_lags}\")\n",
    "\n",
    "        # ACF and PACF Plots\n",
    "        try:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plot_acf(price_diff, lags=(size_product_data//2)-1, title=\"Autocorrelation Function (ACF)\")\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plot_pacf(price_diff, lags=(size_product_data//2)-1, title=\"Partial Autocorrelation Function (PACF)\")\n",
    "            plt.show()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Can't plot the acf/pacf plots for {product_name} due to {e}\")\n",
    "\n",
    "        # Explanation:\n",
    "        # - ACF (Autocorrelation Function): Measures the correlation between a series and its lagged values.\n",
    "        # - PACF (Partial Autocorrelation Function): Measures the correlation between a series and its lagged values,\n",
    "        #   removing the influence of intermediate lags.\n",
    "        # - Use these plots to decide on p (AR term) and q (MA term).\n",
    "        \n",
    "        if p > 5:\n",
    "            p = min(5, q)\n",
    "        if q > 5: \n",
    "            q = min(5, p)\n",
    "        if d > 3:\n",
    "            d = 1\n",
    "\n",
    "        fit_successful = False\n",
    "\n",
    "        while not fit_successful and q >= 0:\n",
    "            try:\n",
    "                print(f\"\\nFitting ARIMA model with order ({p}, {d}, {q})...\")\n",
    "                model = ARIMA(price_data, order=(p, d, q))\n",
    "                model_fit = model.fit()\n",
    "                fit_successful = True  \n",
    "                print(\"ARIMA model fitted successfully!\")\n",
    "                \n",
    "            except np.linalg.LinAlgError as err:\n",
    "                print(f\"Error encountered: {err}\")\n",
    "                if q > 0: \n",
    "                    q -= 1\n",
    "                    print(f\"Reducing q to {q} and trying again...\")\n",
    "                else:\n",
    "                    print(\"Unable to fit model after reducing q multiple times. Exiting loop.\")\n",
    "                    break  \n",
    "        \n",
    "        \n",
    "        # Step 5: Analyze Model Summary\n",
    "        print(\"\\nARIMA Model Summary:\")\n",
    "        print(model_fit.summary())\n",
    "        \n",
    "        \n",
    "        product_parameters = get_product_parameters(filename, product_name)\n",
    "        \n",
    "        for item in product_parameters:\n",
    "            p = item['p']\n",
    "            d = item['d']\n",
    "            q = item['q']\n",
    "            \n",
    "        update_product_parameters(filename, today_date, product_name, p, d, q)\n",
    "        \n",
    "        # with open(filename, mode='a', newline='', encoding='utf-8') as file:\n",
    "        #     writer = csv.writer(file)\n",
    "            \n",
    "        #     # Write header row\n",
    "        #     # writer.writerow(fieldnames)\n",
    "        \n",
    "        #     writer.writerow([today_date, product_name, p, d, q])\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # Assume model_fit is the fitted ARIMA model for this product\n",
    "            hashed_name = safe_filename(product)\n",
    "            model_filename = os.path.join(save_dir, f\"arima_model_{hashed_name}.pkl\")\n",
    "            joblib.dump(model_fit, model_filename)\n",
    "            print(f\"Model for {product} saved as {model_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model for {product}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the models on spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.37.29-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore<1.38.0,>=1.37.29 (from boto3)\n",
      "  Downloading botocore-1.37.29-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3)\n",
      "  Downloading s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\ladan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from botocore<1.38.0,>=1.37.29->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\ladan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from botocore<1.38.0,>=1.37.29->boto3) (2.2.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ladan\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.29->boto3) (1.16.0)\n",
      "Downloading boto3-1.37.29-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.6 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 30.7/139.6 kB 1.4 MB/s eta 0:00:01\n",
      "   ----------- --------------------------- 41.0/139.6 kB 667.8 kB/s eta 0:00:01\n",
      "   ----------------- --------------------- 61.4/139.6 kB 550.5 kB/s eta 0:00:01\n",
      "   ------------------------- ------------- 92.2/139.6 kB 585.1 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 112.6/139.6 kB 598.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- 139.6/139.6 kB 517.3 kB/s eta 0:00:00\n",
      "Downloading botocore-1.37.29-py3-none-any.whl (13.5 MB)\n",
      "   ---------------------------------------- 0.0/13.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.5 MB 487.6 kB/s eta 0:00:28\n",
      "   ---------------------------------------- 0.1/13.5 MB 544.7 kB/s eta 0:00:25\n",
      "   ---------------------------------------- 0.1/13.5 MB 521.8 kB/s eta 0:00:26\n",
      "   ---------------------------------------- 0.1/13.5 MB 544.7 kB/s eta 0:00:25\n",
      "   ---------------------------------------- 0.1/13.5 MB 532.5 kB/s eta 0:00:26\n",
      "   ---------------------------------------- 0.2/13.5 MB 544.7 kB/s eta 0:00:25\n",
      "    --------------------------------------- 0.2/13.5 MB 535.8 kB/s eta 0:00:25\n",
      "    --------------------------------------- 0.2/13.5 MB 497.6 kB/s eta 0:00:27\n",
      "    --------------------------------------- 0.2/13.5 MB 528.4 kB/s eta 0:00:26\n",
      "    --------------------------------------- 0.2/13.5 MB 502.2 kB/s eta 0:00:27\n",
      "    --------------------------------------- 0.3/13.5 MB 519.7 kB/s eta 0:00:26\n",
      "    --------------------------------------- 0.3/13.5 MB 509.3 kB/s eta 0:00:26\n",
      "    --------------------------------------- 0.3/13.5 MB 520.4 kB/s eta 0:00:26\n",
      "   - -------------------------------------- 0.3/13.5 MB 514.3 kB/s eta 0:00:26\n",
      "   - -------------------------------------- 0.4/13.5 MB 512.7 kB/s eta 0:00:26\n",
      "   - -------------------------------------- 0.4/13.5 MB 508.0 kB/s eta 0:00:26\n",
      "   - -------------------------------------- 0.4/13.5 MB 526.8 kB/s eta 0:00:25\n",
      "   - -------------------------------------- 0.5/13.5 MB 531.1 kB/s eta 0:00:25\n",
      "   - -------------------------------------- 0.5/13.5 MB 505.7 kB/s eta 0:00:26\n",
      "   - -------------------------------------- 0.5/13.5 MB 504.5 kB/s eta 0:00:26\n",
      "   - -------------------------------------- 0.5/13.5 MB 519.8 kB/s eta 0:00:25\n",
      "   - -------------------------------------- 0.5/13.5 MB 508.6 kB/s eta 0:00:26\n",
      "   - -------------------------------------- 0.6/13.5 MB 510.6 kB/s eta 0:00:26\n",
      "   - -------------------------------------- 0.6/13.5 MB 509.7 kB/s eta 0:00:26\n",
      "   - -------------------------------------- 0.6/13.5 MB 513.3 kB/s eta 0:00:26\n",
      "   - -------------------------------------- 0.6/13.5 MB 518.8 kB/s eta 0:00:25\n",
      "   - -------------------------------------- 0.7/13.5 MB 516.1 kB/s eta 0:00:25\n",
      "   -- ------------------------------------- 0.7/13.5 MB 521.1 kB/s eta 0:00:25\n",
      "   -- ------------------------------------- 0.7/13.5 MB 525.7 kB/s eta 0:00:25\n",
      "   -- ------------------------------------- 0.7/13.5 MB 522.7 kB/s eta 0:00:25\n",
      "   -- ------------------------------------- 0.7/13.5 MB 518.5 kB/s eta 0:00:25\n",
      "   -- ------------------------------------- 0.8/13.5 MB 522.8 kB/s eta 0:00:25\n",
      "   -- ------------------------------------- 0.8/13.5 MB 525.7 kB/s eta 0:00:25\n",
      "   -- ------------------------------------- 0.8/13.5 MB 524.1 kB/s eta 0:00:25\n",
      "   -- ------------------------------------- 0.9/13.5 MB 523.0 kB/s eta 0:00:25\n",
      "   -- ------------------------------------- 0.9/13.5 MB 525.3 kB/s eta 0:00:24\n",
      "   -- ------------------------------------- 0.9/13.5 MB 521.7 kB/s eta 0:00:25\n",
      "   -- ------------------------------------- 0.9/13.5 MB 520.8 kB/s eta 0:00:25\n",
      "   -- ------------------------------------- 1.0/13.5 MB 529.9 kB/s eta 0:00:24\n",
      "   -- ------------------------------------- 1.0/13.5 MB 526.3 kB/s eta 0:00:24\n",
      "   -- ------------------------------------- 1.0/13.5 MB 525.3 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.0/13.5 MB 531.6 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.1/13.5 MB 535.7 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.1/13.5 MB 528.3 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.1/13.5 MB 527.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.1/13.5 MB 533.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.2/13.5 MB 531.7 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.2/13.5 MB 533.5 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.2/13.5 MB 532.5 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.2/13.5 MB 531.3 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/13.5 MB 536.4 kB/s eta 0:00:23\n",
      "   --- ------------------------------------ 1.3/13.5 MB 533.7 kB/s eta 0:00:23\n",
      "   --- ------------------------------------ 1.3/13.5 MB 525.0 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/13.5 MB 523.3 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 1.3/13.5 MB 525.8 kB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 1.4/13.5 MB 523.4 kB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 1.4/13.5 MB 526.6 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 1.4/13.5 MB 531.8 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 1.5/13.5 MB 534.0 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 1.5/13.5 MB 535.4 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 1.5/13.5 MB 541.1 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 1.6/13.5 MB 558.1 kB/s eta 0:00:22\n",
      "   ---- ----------------------------------- 1.6/13.5 MB 569.9 kB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 1.8/13.5 MB 599.3 kB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 1.9/13.5 MB 624.3 kB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 2.0/13.5 MB 651.9 kB/s eta 0:00:18\n",
      "   ------ --------------------------------- 2.1/13.5 MB 678.9 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 2.1/13.5 MB 675.0 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 2.2/13.5 MB 681.2 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 2.2/13.5 MB 690.3 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 2.3/13.5 MB 699.5 kB/s eta 0:00:17\n",
      "   ------ --------------------------------- 2.4/13.5 MB 711.2 kB/s eta 0:00:16\n",
      "   ------- -------------------------------- 2.4/13.5 MB 722.9 kB/s eta 0:00:16\n",
      "   ------- -------------------------------- 2.5/13.5 MB 731.2 kB/s eta 0:00:16\n",
      "   ------- -------------------------------- 2.7/13.5 MB 774.4 kB/s eta 0:00:14\n",
      "   -------- ------------------------------- 2.8/13.5 MB 802.2 kB/s eta 0:00:14\n",
      "   -------- ------------------------------- 2.9/13.5 MB 835.9 kB/s eta 0:00:13\n",
      "   --------- ------------------------------ 3.1/13.5 MB 876.6 kB/s eta 0:00:12\n",
      "   --------- ------------------------------ 3.3/13.5 MB 924.7 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 3.5/13.5 MB 946.4 kB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 3.6/13.5 MB 981.4 kB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 3.9/13.5 MB 1.0 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 4.0/13.5 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 4.2/13.5 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 4.5/13.5 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 4.7/13.5 MB 1.2 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 4.9/13.5 MB 1.2 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 5.2/13.5 MB 1.3 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 5.2/13.5 MB 1.3 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 5.3/13.5 MB 1.3 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 5.3/13.5 MB 1.3 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 5.4/13.5 MB 1.3 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 5.5/13.5 MB 1.3 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.8/13.5 MB 1.3 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 6.1/13.5 MB 1.3 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 6.2/13.5 MB 1.4 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 6.4/13.5 MB 1.4 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 6.6/13.5 MB 1.4 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 6.9/13.5 MB 1.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 7.0/13.5 MB 1.5 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 7.3/13.5 MB 1.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 7.5/13.5 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 7.7/13.5 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.9/13.5 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 8.1/13.5 MB 1.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 8.3/13.5 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 8.5/13.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.6/13.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.6/13.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.6/13.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.7/13.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.7/13.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.7/13.5 MB 1.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.8/13.5 MB 1.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.8/13.5 MB 1.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.9/13.5 MB 1.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.9/13.5 MB 1.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 9.0/13.5 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 9.2/13.5 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 9.3/13.5 MB 1.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 9.4/13.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 9.5/13.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 9.7/13.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 9.8/13.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 10.1/13.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 10.3/13.5 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 10.5/13.5 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 10.7/13.5 MB 2.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 11.0/13.5 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 11.1/13.5 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.1/13.5 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 11.2/13.5 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 11.2/13.5 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 11.2/13.5 MB 2.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 11.3/13.5 MB 2.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 11.3/13.5 MB 2.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 11.4/13.5 MB 2.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 11.4/13.5 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.5/13.5 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.5/13.5 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.6/13.5 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.7/13.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.9/13.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.0/13.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.1/13.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.3/13.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.5/13.5 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.8/13.5 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.0/13.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.2/13.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.4/13.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.5/13.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.5/13.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.5/13.5 MB 2.9 MB/s eta 0:00:00\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.11.4-py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 0.0/84.4 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 10.2/84.4 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 30.7/84.4 kB 640.0 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 61.4/84.4 kB 656.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 84.4/84.4 kB 674.3 kB/s eta 0:00:00\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.37.29 botocore-1.37.29 jmespath-1.0.1 s3transfer-0.11.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from dateutil import rrule\n",
    "from datetime import date\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from scipy.stats import boxcox, skew\n",
    "from matplotlib import font_manager as fm\n",
    "import os\n",
    "import joblib\n",
    "import hashlib\n",
    "import csv\n",
    "import boto3\n",
    "import io\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "###------------------------------------------------------------------------------------------------------------------------------\n",
    "# Setup your DigitalOcean Spaces client\n",
    "def get_spaces_client():\n",
    "    return boto3.client(\n",
    "        's3',\n",
    "        region_name=os.getenv(\"SPACES_REGION\"),\n",
    "        endpoint_url=os.getenv(\"SPACES_ENDPOINT_URL\"),\n",
    "        aws_access_key_id=os.getenv(\"SPACES_ACCESS_KEY\"),\n",
    "        aws_secret_access_key=os.getenv(\"SPACES_SECRET_KEY\")\n",
    "    )\n",
    "\n",
    "def dump_model_to_spaces(model, product, category):\n",
    "    try:\n",
    "        # Prepare filename\n",
    "        hashed_name = safe_filename(product)\n",
    "        filename = f\"arima_model_{hashed_name}.pkl\"\n",
    "        key = f\"ml_models/{category}/{filename}\"  # e.g., ml_models/fruits_saved_models/arima_model_apple.pkl\n",
    "        bucket = os.getenv(\"SPACES_BUCKET_NAME\")\n",
    "        print(f\"Uploading to bucket: {bucket} with key: {key}\")\n",
    "\n",
    "        # Serialize model to bytes\n",
    "        bytes_buffer = io.BytesIO()\n",
    "        joblib.dump(model, bytes_buffer)\n",
    "        bytes_buffer.seek(0)\n",
    "\n",
    "        # Upload to Spaces\n",
    "        client = get_spaces_client()\n",
    "        client.put_object(Bucket=os.getenv(\"SPACES_BUCKET_NAME\"), Key=key, Body=bytes_buffer)\n",
    "\n",
    "        print(f\"Model for {product} uploaded to Spaces at {key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading model for {product} to Spaces: {e}\")\n",
    "        \n",
    "\n",
    "def filterData(existing_data):\n",
    "    data = existing_data\n",
    "    \n",
    "    # Get current date and calculate the cutoff date (4 months ago)\n",
    "    current_date = datetime.today()\n",
    "    cutoff_date = current_date - timedelta(days=4*30) # Approximate 4 months\n",
    "    cutoff_date = cutoff_date.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "    # today_date = datetime.now().strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    # Filter data to keep only the last 4 months\n",
    "    filtered_data = data[pd.to_datetime(data[\"Date\"], dayfirst=True) >= cutoff_date]\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def append_csv_to_spaces(new_df: pd.DataFrame, filename: str):\n",
    "    client = get_spaces_client()\n",
    "    key = f\"data/fruits/{filename}\"\n",
    "\n",
    "    try:\n",
    "        # Step 1: Download existing file if it exists\n",
    "        existing = client.get_object(Bucket=os.getenv(\"SPACES_BUCKET_NAME\"), Key=key)\n",
    "        existing_data = pd.read_csv(existing['Body'])\n",
    "        print(\"📥 Existing CSV loaded from Spaces.\")\n",
    "        \n",
    "        if(filename == \"fruits_price_data.csv\"):\n",
    "            existing_data = filterData(existing_data)\n",
    "            \n",
    "\n",
    "        # Step 2: Append new data\n",
    "        combined_df = pd.concat([existing_data, new_df], ignore_index=True)\n",
    "\n",
    "    except client.exceptions.NoSuchKey:\n",
    "        print(\"ℹ️ No existing file found. Creating a new one.\")\n",
    "        combined_df = new_df\n",
    "\n",
    "    # Step 3: Upload updated CSV\n",
    "    csv_buffer = io.StringIO()\n",
    "    combined_df.to_csv(csv_buffer, index=False)\n",
    "    csv_buffer.seek(0)\n",
    "\n",
    "    client.put_object(Bucket=os.getenv(\"SPACES_BUCKET_NAME\"), Key=key, Body=csv_buffer.getvalue())\n",
    "\n",
    "    print(f\"✅ CSV saved to Spaces at {key}\")\n",
    "\n",
    "\n",
    "\n",
    "products_with_boxcox = []\n",
    "### removes data older than 4 months-----------------------------------------------------------------------------------------------\n",
    "base_dir = Path(os.getenv(\"BASE_DIRECTORY\"))\n",
    "file_path = base_dir / \"data/fruits/fruits_price_data.csv\"\n",
    "\n",
    "\n",
    "# csv_file = \"commodities_price_data.csv\"  # Update with your file path\n",
    "save_dir = base_dir / \"ml_models/fruits_saved_models\"\n",
    "filename = base_dir / \"ml_models/fruits_saved_models/fruits_parameters.csv\"\n",
    "allYearsFilename = base_dir / \"data/fruits/fruits_price_data-all_years.csv\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "data = pd.read_csv(file_path, encoding='utf-8', parse_dates=['Date'], date_format='%d/%m/%Y')\n",
    "print(data.head())\n",
    "products = data['Item Name'].unique()\n",
    "print(products)\n",
    "\n",
    "# Get current date and calculate the cutoff date (4 months ago)\n",
    "current_date = datetime.today()\n",
    "cutoff_date = current_date - timedelta(days=4*30) # Approximate 4 months\n",
    "cutoff_date = cutoff_date.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "today_date = datetime.now().strftime(\"%d/%m/%Y\")\n",
    "\n",
    "# Filter data to keep only the last 4 months\n",
    "filtered_data = data[pd.to_datetime(data[\"Date\"], dayfirst=True) >= cutoff_date]\n",
    "\n",
    "# Save the cleaned data back to CSV\n",
    "filtered_data.to_csv(file_path, encoding='utf-8', index=False)\n",
    "\n",
    "print(f\"Data older than {cutoff_date.strftime('%d-%m-%Y')} has been removed.\")\n",
    "\n",
    "logs = []\n",
    "###------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "###Saving updated ARIMA models---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Step 1: Stationarity Check\n",
    "def check_stationarity(series, size):\n",
    "    if len(series) == size:\n",
    "        return True\n",
    "    result = adfuller(series)\n",
    "    print(\"\\nADF Test Results:\")\n",
    "    print(f\"ADF Statistic: {result[0]:.4f}\")\n",
    "    print(f\"p-value: {result[1]:.4f}\")\n",
    "    for key, value in result[4].items():\n",
    "        print(f\"Critical Value {key}: {value:.4f}\")\n",
    "    if result[1] < 0.05:\n",
    "        print(\"The series is stationary (no further differencing needed).\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"The series is not stationary (differencing is required).\")\n",
    "        return False\n",
    "\n",
    "# Function to count significant lags\n",
    "def count_significant_spikes(values, confint):\n",
    "    # The significant lags are those outside the confidence interval\n",
    "    significant_lags = np.where(np.abs(values) > confint[:, 1])[0]\n",
    "    return len(significant_lags), significant_lags\n",
    "\n",
    "# Function to generate a safe file name\n",
    "def safe_filename(product_name):\n",
    "    return hashlib.md5(product_name.encode('utf-8')).hexdigest()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###updating the csv with current data---------------------------------------------------------------------------------------------\n",
    "\n",
    "def write_to_csv(data, filename):\n",
    "    # Define the field names for the CSV file\n",
    "    # fieldnames = ['Serial Number', 'Date', 'Item Name', 'Low Price', 'Average Price', 'High Price']\n",
    "    \n",
    "    # Write data to CSV file\n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write header row\n",
    "        # writer.writerow(fieldnames)\n",
    "        \n",
    "        # Write data rows\n",
    "        for _, row_data in enumerate(data):\n",
    "            date = row_data[0]\n",
    "            item_name = row_data[2]\n",
    "            low_price = row_data[3]\n",
    "            avg_price = row_data[5]\n",
    "            high_price = row_data[4]\n",
    "            writer.writerow([date, item_name, low_price, avg_price, high_price])\n",
    "            \n",
    "            \n",
    "def getData(driver, csv_file_name, webpage_path, date_path, submit_button_path, table_path):\n",
    "    \n",
    "    # add_field_names_price(csv_file_name)\n",
    "    \n",
    "    #iterate over all the dates    \n",
    "    try:\n",
    "        # Open the webpage\n",
    "        driver.get(webpage_path)\n",
    "            \n",
    "        # Find the date input element\n",
    "        date_input = driver.find_element(By.XPATH, date_path)\n",
    "\n",
    "        # Clear any existing text in the date input field\n",
    "        date_input.clear()\n",
    "          \n",
    "        # Enter the desired date\n",
    "        date_input.send_keys(current_date.strftime('%d-%m-%Y'))  # Example date, replace with your desired date\n",
    "\n",
    "        # Find and click the button to get data for the specified date\n",
    "        submit_button = driver.find_element(By.XPATH, submit_button_path)\n",
    "        submit_button.click()\n",
    "         \n",
    "        time.sleep(3) #to load data\n",
    "\n",
    "        #Extract the table data\n",
    "        table = driver.find_element(By.XPATH, table_path)\n",
    "        # print(table)\n",
    "        data = []\n",
    "\n",
    "        # Iterate over each row in the table\n",
    "        for row in table.find_elements(By.TAG_NAME, \"tr\"):\n",
    "            # Find all cells (td) in the row\n",
    "            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                \n",
    "            # Ensure that there are cells in the row\n",
    "            if cells:\n",
    "                # Extract text from each cell and append to the data list\n",
    "                row_data = [current_date.strftime('%d-%m-%Y')] + [cell.text.strip() for cell in cells]\n",
    "                data.append(row_data)\n",
    "        \n",
    "        \n",
    "        columns = [\"Date\", \"Some Extra Column\", \"Item Name\", \"Low Price\", \"Average Price\", \"High Price\"]\n",
    "        new_df = pd.DataFrame(data, columns=columns)\n",
    "        print(\"Before dropping the column:\")\n",
    "        print(new_df.head())\n",
    "        \n",
    "\n",
    "        # Drop the column at index 1 (which corresponds to \"Item Name\")\n",
    "        new_df.drop(new_df.columns[1], axis=1, inplace=True)\n",
    "        \n",
    "        print(\"\\nAfter dropping the column:\")\n",
    "        print(new_df.head())\n",
    "        \n",
    "        new_df['Average Price'], new_df['High Price'] = new_df['High Price'], new_df['Average Price']\n",
    "\n",
    "        # Print the new DataFrame to verify\n",
    "        print(\"\\nAfter rearranging the column:\")\n",
    "        print(new_df.head())\n",
    "\n",
    "\n",
    "        # Print the extracted data for debugging\n",
    "        # for row in data:\n",
    "        #     print(row)\n",
    "        write_to_csv(data, csv_file_name)\n",
    "        append_csv_to_spaces(new_df, \"fruits_price_data.csv\")\n",
    "        write_to_csv(data, allYearsFilename)\n",
    "        append_csv_to_spaces(new_df, \"fruits_price_data-all_years.csv\")\n",
    "        time.sleep(2)\n",
    "        # return data\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Initialize Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # commodities_data_table = \n",
    "    getData(driver, file_path, os.getenv(\"FRUITS_WEBPAGE_PATH\"),\n",
    "                                           os.getenv(\"FRUITS_DATE_PATH\"),\n",
    "                                           os.getenv(\"FRUITS_SUBMIT_BUTTON_PATH\"),\n",
    "                                           os.getenv(\"FRUITS_TABLE_PATH\"))\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###Updating models------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    for product in products:\n",
    "    \n",
    "        product_name = product \n",
    "        product_data = data[data['Item Name'] == product_name]\n",
    "        size_product_data = product_data.shape[0]\n",
    "        print(product_name, \"size: \", size_product_data)\n",
    "        \n",
    "        if((size_product_data//2)-1 <= 0):\n",
    "            continue\n",
    "        \n",
    "        # Check if any data exists for the selected product\n",
    "        if product_data.empty:\n",
    "            print(f\"No data found for the product: {product_name}\")\n",
    "\n",
    "        else:\n",
    "            # Extract the 'Average Price' column for the selected product\n",
    "            price_data = product_data['Average Price']\n",
    "            \n",
    "        variance_ratio = price_data.std() / price_data.mean()\n",
    "        skewness = skew(price_data)\n",
    "\n",
    "        if variance_ratio > 0.1 and skewness > 0.5:\n",
    "            apply_boxcox = True\n",
    "            products_with_boxcox.append(product_name)\n",
    "        else:\n",
    "            apply_boxcox = False\n",
    "        \n",
    "        \n",
    "        print(apply_boxcox)\n",
    "        # Step 2: Differencing (if necessary)\n",
    "        print(f\"Checking stationarity for the raw price data of {product_name}:\")\n",
    "        d = 0\n",
    "        price_diff = price_data\n",
    "        \n",
    "        try:\n",
    "            if apply_boxcox:\n",
    "                price_diff, lam = boxcox(price_diff)\n",
    "                price_diff = pd.Series(price_diff)\n",
    "        \n",
    "            while True:\n",
    "                if check_stationarity(price_diff, (size_product_data//2)+1):\n",
    "                    # price_diff = price_data\n",
    "                    print(f\"Data is stationary with d={d}.\")\n",
    "                    break\n",
    "                else:\n",
    "                    price_diff = price_diff.diff().dropna()\n",
    "                    d += 1\n",
    "        \n",
    "\n",
    "            print(\"\\nChecking stationarity for the differenced data:\")\n",
    "            check_stationarity(price_diff, (size_product_data//2)+1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Stationarity cannot be applicable for {product_name}\")\n",
    "        \n",
    "        \n",
    "        # Step 3: Plot ACF and PACF\n",
    "\n",
    "        acf_values, acf_confint = acf(price_diff, alpha=0.05)\n",
    "        pacf_values, pacf_confint = pacf(price_diff, alpha=0.05)\n",
    "\n",
    "\n",
    "        # Count significant spikes for ACF (q)\n",
    "        q, significant_acf_lags = count_significant_spikes(acf_values, acf_confint)\n",
    "\n",
    "        # Count significant spikes for PACF (p)\n",
    "        p, significant_pacf_lags = count_significant_spikes(pacf_values, pacf_confint)\n",
    "\n",
    "        print(f\"Significant spikes in ACF (q): {q} at lags {significant_acf_lags}\")\n",
    "        print(f\"Significant spikes in PACF (p): {p} at lags {significant_pacf_lags}\")\n",
    "\n",
    "        # ACF and PACF Plots\n",
    "        try:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plot_acf(price_diff, lags=(size_product_data//2)-1, title=\"Autocorrelation Function (ACF)\")\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plot_pacf(price_diff, lags=(size_product_data//2)-1, title=\"Partial Autocorrelation Function (PACF)\")\n",
    "            plt.show()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Can't plot the acf/pacf plots for {product_name} due to {e}\")\n",
    "        \n",
    "        if p > 5:\n",
    "            p = min(5, q)\n",
    "        if q > 5: \n",
    "            q = min(5, p)\n",
    "        if d > 3:\n",
    "            d = 1\n",
    "\n",
    "        fit_successful = False\n",
    "\n",
    "        while not fit_successful and q >= 0:\n",
    "            try:\n",
    "                print(f\"\\nFitting ARIMA model with order ({p}, {d}, {q})...\")\n",
    "                model = ARIMA(price_data, order=(p, d, q))\n",
    "                model_fit = model.fit()\n",
    "                fit_successful = True  \n",
    "                print(\"ARIMA model fitted successfully!\")\n",
    "                \n",
    "            except np.linalg.LinAlgError as err:\n",
    "                print(f\"Error encountered: {err}\")\n",
    "                if q > 0: \n",
    "                    q -= 1\n",
    "                    print(f\"Reducing q to {q} and trying again...\")\n",
    "                else:\n",
    "                    print(\"Unable to fit model after reducing q multiple times. Exiting loop.\")\n",
    "                    break  \n",
    "        \n",
    "        \n",
    "        # Step 5: Analyze Model Summary\n",
    "        print(\"\\nARIMA Model Summary:\")\n",
    "        print(model_fit.summary())\n",
    "        \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # Assume model_fit is the fitted ARIMA model for this product\n",
    "            dump_model_to_spaces(model_fit, product, \"fruits_saved_models\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model for {product}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
